{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5  …  10.5, 11.0, 11.5, 12.0, 12.5, 13.0, 13.5, 14.0, 14.5, 15.0], [-7.847359573362729, -1.3315947853190484, -0.5803510819209906, 1.1102764392598465, 9.988712842023517, 19.064552472027945, 22.360242601879634, 33.29037569667121, 44.30268624603188, 61.37029298584822  …  329.46633376403497, 367.3951090879816, 401.4586394961345, 429.0802630665355, 468.61496107560146, 507.32002150234865, 545.8993822489858, 589.8509862829196, 640.5342047989809, 675.5419963803967], [2.3730589643393882, 1.3446357078635467, 0.21190807702627767, 2.4126456988021374, 1.4574964737150893, 4.5564066433504316, 2.007971577637596, 1.5027663187179967, 2.6671045028916103, 1.8335143799268723  …  2.4187475006856918, 3.7358953477686847, 3.7144608329447912, 4.750626684174312, 0.10958338163060743, 4.894885896061309, 0.732250812640492, 2.748137898114247, 4.061190017100702, 0.033932572914373016])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LogDensityProblems: LogDensityProblems;\n",
    "using Distributions\n",
    "using DelimitedFiles\n",
    "using Random: Random\n",
    "using DynamicPPL\n",
    "\n",
    "data = readdlm(\"data.txt\")\n",
    "t, y, yerr = data[:, 1], data[:, 2], data[:, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define some type that represents the model.\n",
    "struct RegressionProblem{Ty <: AbstractVector}\n",
    "\t\"mean of the isotropic Gaussian\"\n",
    "\ty::Ty\n",
    "\tx::Ty\n",
    "\t# α::A\n",
    "\t# β::A\n",
    "\t# γ::A\n",
    "end\n",
    "LogDensityProblems.dimension(model::RegressionProblem) = 3\n",
    "\n",
    "function LogDensityProblems.logdensity(model::RegressionProblem, θ::AbstractVector{<:Real})\n",
    "\tα, β, γ = θ\n",
    "\tlp = logpdf(Normal(2, 1), α)\n",
    "\tlp += logpdf(Normal(0, 1), β)\n",
    "\tlp += logpdf(Normal(-2, 1), γ)\n",
    "\tll = logpdf(MvNormal(fun(model.x, α, β, γ), 1), model.y)\n",
    "\treturn ll + lp\n",
    "end\n",
    "\n",
    "\n",
    "fun(x,α,β,γ) = @. α * x^2 + β * x + γ\n",
    "x = 0.0:0.5:15.0\n",
    "\n",
    "LogDensityProblems.capabilities(model::RegressionProblem) = LogDensityProblems.LogDensityOrder{0}()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressionProblem{Vector{Float64}}([-7.847359573362729, -1.3315947853190484, -0.5803510819209906, 1.1102764392598465, 9.988712842023517, 19.064552472027945, 22.360242601879634, 33.29037569667121, 44.30268624603188, 61.37029298584822  …  329.46633376403497, 367.3951090879816, 401.4586394961345, 429.0802630665355, 468.61496107560146, 507.32002150234865, 545.8993822489858, 589.8509862829196, 640.5342047989809, 675.5419963803967], [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5  …  10.5, 11.0, 11.5, 12.0, 12.5, 13.0, 13.5, 14.0, 14.5, 15.0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MYmodel = RegressionProblem(y,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DifferentialMove"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function tune_lengthscale(t, μ, N_e, N_c, M_adapt)\n",
    "\tN_e = max(1, N_e)\n",
    "\n",
    "\tif t <= M_adapt\n",
    "\t\treturn 2μ * N_e / (N_e + N_c)\n",
    "\telse\n",
    "\t\treturn μ\n",
    "\tend\n",
    "end\n",
    "\n",
    "function get_complementary(i, N)\n",
    "\tindices = collect(1:N)\n",
    "\tdeleteat!(indices, i)\n",
    "\treturn indices\n",
    "end\n",
    "\n",
    "function get_direction_vector(S, l, m, μ)\n",
    "\treturn μ * (S[l].x - S[m].x)\n",
    "end\n",
    "\n",
    "\"\"\" \n",
    "\tDifferentialMove(rng, k, μ, S, N)\n",
    "\n",
    "\tPerform a differential move for walker k.\n",
    "\n",
    "\t# Arguments\n",
    "\t- `rng::AbstractRNG`: Random number generator.\n",
    "\t- `k::Int`: Index of the walker.\n",
    "\t- `μ::Float64`: Lengthscale.\n",
    "\t- `S::Array{Float64, 2}`: Array of walker positions.\n",
    "\t- `N::Int`: Number of walkers.\n",
    "\n",
    "\t# Returns\n",
    "\t- `ηₖ::Array{Float64, 1}`: Differential move.\n",
    "\"\"\"\n",
    "function DifferentialMove(rng, k, μ, S, N)\n",
    "\t# work on walker k\n",
    "\tindices = get_complementary(k, N)\n",
    "\t# draw two random indices from the complementary set, without replacement\n",
    "\tl, m = sample(rng, indices, 2, replace = false)\n",
    "\treturn get_direction_vector(S, l, m, μ)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "using AbstractMCMC\n",
    "\n",
    "struct EnsembleSliceSampler{T<:Float64,A<:Int64} <: AbstractMCMC.AbstractSampler\n",
    "    \"initial length scale\"\n",
    "    μ_init::T\n",
    "    \"number of adapation steps\"\n",
    "    M_adapt::A\n",
    "    \"number of walkers\"\n",
    "    N_walkers::A\n",
    "    \"max number of attempts\"\n",
    "    max_steps::A\n",
    "end\n",
    "struct Walker{A<:AbstractVector{<:Real}}\n",
    "    \"current position\"\n",
    "    x::A # a matrix of dimension n_params\n",
    "end\n",
    "\n",
    "struct ESState{A<:AbstractVector,T<:Float64,B<:Int64}\n",
    "    # \"current position\"\n",
    "    # vi::V\n",
    "    \"current position\"\n",
    "    x::A\n",
    "    \"length scale\"\n",
    "    μ::T\n",
    "    \"iteration\"\n",
    "    t::B\n",
    "end\n",
    "\n",
    "struct ESSample{A<:AbstractVector}\n",
    "    \"current position\"\n",
    "    x::A # a matrix of dimension n_walkers * n_params\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "function AbstractMCMC.step(\n",
    "\trng::Random.AbstractRNG,\n",
    "\tmodel_wrapper::AbstractMCMC.LogDensityModel,\n",
    "\tsampler::EnsembleSliceSampler,\n",
    "\tstate::ESState)\n",
    "\n",
    "\tmodel = model_wrapper.logdensity\n",
    "\t# extract the sampler parameters\n",
    "\tμ = sampler.μ_init\n",
    "\tM_adapt = sampler.M_adapt\n",
    "\tN_walkers = sampler.N_walkers\n",
    "\tmax_steps = sampler.max_steps\n",
    "\n",
    "\tf(y) = LogDensityProblems.logdensity(model, y)\n",
    "\n",
    "\t# extract current state\n",
    "\tS, μ, t = state.x, state.μ, state.t\n",
    "\tN_dim = size(x, 2)\n",
    "\n",
    "\tx_new = Vector(undef, N_walkers)\n",
    "\t# Matrix{Float64}(undef, N_walkers, N_dim)\n",
    "\n",
    "\tR, L, N_e, N_c = 0, 0, 0, 0\n",
    "\tX′ = 0\n",
    "\n",
    "\t# loop over the walkers\n",
    "\tfor k in 1:N_walkers\n",
    "\t\tXₖ = S[k].x # get the current position of walker k\n",
    "\t\tηₖ = DifferentialMove(rng, k, μ, S, N_walkers) # get the differential move\n",
    "\n",
    "\t\tδ = rand(rng, Exponential(1))\n",
    "\t\tY = f(Xₖ) - δ\n",
    "\n",
    "\t\tL = -rand(rng)\n",
    "\t\tR = L + 1\n",
    "\t\tl = 0\n",
    "\t\twhile Y < f(L .* ηₖ + Xₖ)\n",
    "\t\t\tL = L - 1\n",
    "\t\t\tN_e = N_e + 1\n",
    "\t\t\tl += 1\n",
    "\t\t\tif l == max_steps\n",
    "\t\t\t\tprintln(\"L: \", L, \" Y: \", Y, \" f(L): \", f(L .* ηₖ + Xₖ))\n",
    "\t\t\t\terror(\"Max steps reached\", \" iteration: \", t, \" walker: \", k)\n",
    "\t\t\tend\n",
    "\t\tend\n",
    "\t\tl = 0\n",
    "\t\twhile Y < f(R .* ηₖ + Xₖ)\n",
    "\t\t\tR = R + 1\n",
    "\t\t\tN_e = N_e + 1\n",
    "\t\t\tl += 1\n",
    "\t\t\tif l == max_steps\n",
    "\t\t\t\tprintln(\"L: \", R, \" Y: \", Y, \" f(R): \", f(R .* ηₖ + Xₖ))\n",
    "\t\t\t\terror(\"Max steps reached\")\n",
    "\t\t\tend\n",
    "\t\tend\n",
    "\n",
    "\t\tl = 0\n",
    "\t\twhile true\n",
    "\t\t\tl += 1\n",
    "\t\t\tX′ = rand(rng, Uniform(L, R))\n",
    "\t\t\tY′ = f(X′ .* ηₖ + Xₖ)\n",
    "\t\t\tif Y < Y′\n",
    "\t\t\t\tbreak\n",
    "\t\t\tend\n",
    "\t\t\tif X′ < 0\n",
    "\t\t\t\tL = X′\n",
    "\t\t\t\tN_c = N_c + 1\n",
    "\t\t\telse\n",
    "\t\t\t\tR = X′\n",
    "\t\t\t\tN_c = N_c + 1\n",
    "\t\t\tend\n",
    "\t\t\tif l == max_steps\n",
    "\t\t\t\tprintln(\"L: \", R, \" Y: \", Y, \" f(R): \", f(R .* ηₖ + Xₖ))\n",
    "\n",
    "\t\t\t\terror(\"Max steps reached\")\n",
    "\t\t\tend\n",
    "\t\tend\n",
    "\n",
    "\t\tXₖ = X′ .* ηₖ + Xₖ\n",
    "\t\tx_new[k] = Walker(Xₖ)\n",
    "\t\t# push!(x_new, Walker(Xₖ))\n",
    "\tend\n",
    "\t# println(\"R: \", R, \" L: \", L, \" N_e: \", N_e, \" N_c: \", N_c, \" μ: \", μ)\n",
    "\tμ = tune_lengthscale(t, μ, N_e, N_c, M_adapt)\n",
    "\tt += 1\n",
    "\t# println((x_new) isa Vector{<:Walker})\n",
    "\tstate_new = ESState(x_new, μ, t)\n",
    "\treturn ESSample(x_new), state_new\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESState{Vector{Walker{Vector{Float64}}}, Float64, Int64}(Walker{Vector{Float64}}[Walker{Vector{Float64}}([-1.4700350877845583, -0.28079717563588386, 0.21819935075366134]), Walker{Vector{Float64}}([0.0015359654656915698, -1.292041156524825, -0.5001426097687957]), Walker{Vector{Float64}}([1.6207908277595857, 0.6888362954156618, -1.3282856773114313]), Walker{Vector{Float64}}([0.5969900613470707, 1.5204721141867714, -1.1102535821792976]), Walker{Vector{Float64}}([0.555143907575193, -2.7896723633694007, 0.9822429048406388]), Walker{Vector{Float64}}([1.2773868591229607, 1.6377902315278676, 0.09285335904755491])], 1.0, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = Random.default_rng(89)\n",
    "ndims = 3\n",
    "nwalkers = 2 * ndims\n",
    "sampler = EnsembleSliceSampler(1.0, 100, 6, 10_000)\n",
    "init_p = [ Walker(randn(rng,ndims)) for i in 1:nwalkers]\n",
    "state = ESState(init_p, 1.0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ESSample{Vector{Any}}(Any[Walker{Vector{Float64}}([-1.320428073770339, 0.6363172898812206, 0.033969218834172865]), Walker{Vector{Float64}}([3.7951771327792407, -0.10192513139156234, -2.398279039127977]), Walker{Vector{Float64}}([2.4451932864233825, 1.407245953793178, -1.8581195134353679]), Walker{Vector{Float64}}([1.982880798480703, 3.2158690377767565, -1.8190461466202503]), Walker{Vector{Float64}}([2.2543304577792065, -1.3089489421531624, -0.10980445470264522]), Walker{Vector{Float64}}([2.427923317230537, 7.072128628470728, -1.0860031172199034])]), ESState{Vector{Any}, Float64, Int64}(Any[Walker{Vector{Float64}}([-1.320428073770339, 0.6363172898812206, 0.033969218834172865]), Walker{Vector{Float64}}([3.7951771327792407, -0.10192513139156234, -2.398279039127977]), Walker{Vector{Float64}}([2.4451932864233825, 1.407245953793178, -1.8581195134353679]), Walker{Vector{Float64}}([1.982880798480703, 3.2158690377767565, -1.8190461466202503]), Walker{Vector{Float64}}([2.2543304577792065, -1.3089489421531624, -0.10980445470264522]), Walker{Vector{Float64}}([2.427923317230537, 7.072128628470728, -1.0860031172199034])], 1.8333333333333333, 2))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = Random.default_rng(132)\n",
    "\n",
    "x_next, state_next = AbstractMCMC.step(\n",
    "    rng,\n",
    "    AbstractMCMC.LogDensityModel(MYmodel),\n",
    "    sampler,\n",
    "    state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DynamicPPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "function AbstractMCMC.step(\n",
    "    rng::Random.AbstractRNG,\n",
    "    model_wrapper::AbstractMCMC.LogDensityModel,\n",
    "    ::EnsembleSliceSampler;\n",
    "    kwargs...)\n",
    "    model = model_wrapper.logdensity\n",
    "    nwalkers = sampler.N_walkers\n",
    "    ndims = LogDensityProblems.dimension(model)\n",
    "    # x = randn(rng,nwalkers,ndims)\n",
    "    init_p = [ Walker(randn(rng,ndims)) for i in 1:nwalkers]\n",
    "    # x = [DynamicPPL.VarInfo(rng, model, DynamicPPL.SampleFromPrior()) for _ in 1:nwalkers]\n",
    "    # x = mapreduce(permutedims, hcat, x)\n",
    "    return ESSample(init_p),  ESState(init_p, 1.0, 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# samples = sample(MYmodel, sampler, 1_000; initial_state=state, progress=false)\n",
    "# using MCMCChains\n",
    "# samples_matrix = stack(sample -> sample.x, samples)\n",
    "# samples_matrix = stack(sample -> sample.x, samples_matrix)\n",
    "# samples_matrix = permutedims(samples_matrix, [3, 1, 2])\n",
    "\n",
    "# ch = Chains(samples_matrix,[:α,:β,:γ])\n",
    "# using StatsPlots\n",
    "# plot(ch[100:end,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "@model function InferenceModel(y, t)\n",
    "\tα ~ Normal(2, 1)\n",
    "\tβ ~ Normal(0, 1)\n",
    "\tγ ~ Normal(-2, 1)\n",
    "\treturn y ~ MvNormal(fun(t, α, β, γ), I)\n",
    "end\n",
    "\n",
    "mymod = InferenceModel(y, t)\n",
    "# Turing.Inference.getparams(::Turing.Model, sample::ESSample) = sample.x\n",
    "function Turing.Inference.getparams(::Turing.Model, sample::ESSample) \n",
    "\t# for walker in sample.x\n",
    "\t# \tprintln(walker.x)\n",
    "\t# end\n",
    "\treturn [sample.x[i].x for i in 1:length(sample.x)]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TypedVarInfo{@NamedTuple{α::DynamicPPL.Metadata{Dict{VarName{:α, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:α, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}, β::DynamicPPL.Metadata{Dict{VarName{:β, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:β, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}, γ::DynamicPPL.Metadata{Dict{VarName{:γ, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:γ, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}}, Float64}((α = DynamicPPL.Metadata{Dict{VarName{:α, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:α, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}(Dict(α => 1), [α], UnitRange{Int64}[1:1], [0.4993117530487674], Normal{Float64}[Normal{Float64}(μ=2.0, σ=1.0)], Set{DynamicPPL.Selector}[Set()], [0], Dict{String, BitVector}(\"del\" => [0], \"trans\" => [0])), β = DynamicPPL.Metadata{Dict{VarName{:β, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:β, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}(Dict(β => 1), [β], UnitRange{Int64}[1:1], [-0.2473210977555573], Normal{Float64}[Normal{Float64}(μ=0.0, σ=1.0)], Set{DynamicPPL.Selector}[Set()], [0], Dict{String, BitVector}(\"del\" => [0], \"trans\" => [0])), γ = DynamicPPL.Metadata{Dict{VarName{:γ, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:γ, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}(Dict(γ => 1), [γ], UnitRange{Int64}[1:1], [-0.7033244738735214], Normal{Float64}[Normal{Float64}(μ=-2.0, σ=1.0)], Set{DynamicPPL.Selector}[Set()], [0], Dict{String, BitVector}(\"del\" => [0], \"trans\" => [0]))), Base.RefValue{Float64}(-1.0544915847023195e6), Base.RefValue{Int64}(1))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vi = DynamicPPL.VarInfo(rng, mymod, DynamicPPL.SampleFromPrior()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Turing.Inference.Transition"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Turing.Inference.Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler = Emcee(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain = sample(mymod, sampler, 10_00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mSampling   0%|                                          |  ETA: N/A\u001b[39m\n",
      "\u001b[90mSampling 100%|██████████████████████████████████████████| Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching DynamicPPL.Metadata(::Dict{VarName{:α, typeof(identity)}, Int64}, ::Vector{VarName{:α, typeof(identity)}}, ::Vector{UnitRange{Int64}}, ::Vector{Vector{Float64}}, ::Vector{Normal{Float64}}, ::Vector{Set{DynamicPPL.Selector}}, ::Vector{Int64}, ::Dict{String, BitVector})\n\nClosest candidates are:\n  DynamicPPL.Metadata(::TIdcs, ::TVN, ::Vector{UnitRange{Int64}}, !Matched::TVal, ::TDists, ::TGIds, ::Vector{Int64}, ::Dict{String, BitVector}) where {TIdcs<:(Dict{<:VarName, Int64}), TDists<:(AbstractVector{<:Distribution}), TVN<:(AbstractVector{<:VarName}), TVal<:(AbstractVector{<:Real}), TGIds<:AbstractVector{Set{DynamicPPL.Selector}}}\n   @ DynamicPPL ~/.julia/packages/DynamicPPL/DvdZw/src/varinfo.jl:47\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching DynamicPPL.Metadata(::Dict{VarName{:α, typeof(identity)}, Int64}, ::Vector{VarName{:α, typeof(identity)}}, ::Vector{UnitRange{Int64}}, ::Vector{Vector{Float64}}, ::Vector{Normal{Float64}}, ::Vector{Set{DynamicPPL.Selector}}, ::Vector{Int64}, ::Dict{String, BitVector})\n",
      "\n",
      "Closest candidates are:\n",
      "  DynamicPPL.Metadata(::TIdcs, ::TVN, ::Vector{UnitRange{Int64}}, !Matched::TVal, ::TDists, ::TGIds, ::Vector{Int64}, ::Dict{String, BitVector}) where {TIdcs<:(Dict{<:VarName, Int64}), TDists<:(AbstractVector{<:Distribution}), TVN<:(AbstractVector{<:VarName}), TVal<:(AbstractVector{<:Real}), TGIds<:AbstractVector{Set{DynamicPPL.Selector}}}\n",
      "   @ DynamicPPL ~/.julia/packages/DynamicPPL/DvdZw/src/varinfo.jl:47\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      "  [1] macro expansion\n",
      "    @ ~/.julia/packages/DynamicPPL/DvdZw/src/varinfo.jl:0 [inlined]\n",
      "  [2] newmetadata(metadata::@NamedTuple{α::DynamicPPL.Metadata{Dict{VarName{:α, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:α, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}, β::DynamicPPL.Metadata{Dict{VarName{:β, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:β, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}, γ::DynamicPPL.Metadata{Dict{VarName{:γ, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:γ, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}}, ::Val{()}, x::Vector{Vector{Float64}})\n",
      "    @ DynamicPPL ~/.julia/packages/DynamicPPL/DvdZw/src/varinfo.jl:158\n",
      "  [3] VarInfo(old_vi::TypedVarInfo{@NamedTuple{α::DynamicPPL.Metadata{Dict{VarName{:α, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:α, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}, β::DynamicPPL.Metadata{Dict{VarName{:β, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:β, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}, γ::DynamicPPL.Metadata{Dict{VarName{:γ, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:γ, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}}, Float64}, spl::SampleFromPrior, x::Vector{Vector{Float64}})\n",
      "    @ DynamicPPL ~/.julia/packages/DynamicPPL/DvdZw/src/varinfo.jl:116\n",
      "  [4] unflatten(vi::TypedVarInfo{@NamedTuple{α::DynamicPPL.Metadata{Dict{VarName{:α, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:α, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}, β::DynamicPPL.Metadata{Dict{VarName{:β, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:β, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}, γ::DynamicPPL.Metadata{Dict{VarName{:γ, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:γ, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}}, Float64}, spl::SampleFromPrior, x::Vector{Vector{Float64}})\n",
      "    @ DynamicPPL ~/.julia/packages/DynamicPPL/DvdZw/src/varinfo.jl:137\n",
      "  [5] unflatten(vi::TypedVarInfo{@NamedTuple{α::DynamicPPL.Metadata{Dict{VarName{:α, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:α, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}, β::DynamicPPL.Metadata{Dict{VarName{:β, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:β, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}, γ::DynamicPPL.Metadata{Dict{VarName{:γ, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:γ, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}}, Float64}, x::Vector{Vector{Float64}})\n",
      "    @ DynamicPPL ~/.julia/packages/DynamicPPL/DvdZw/src/varinfo.jl:134\n",
      "  [6] transition_to_turing(f::LogDensityFunction{TypedVarInfo{@NamedTuple{α::DynamicPPL.Metadata{Dict{VarName{:α, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:α, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}, β::DynamicPPL.Metadata{Dict{VarName{:β, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:β, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}, γ::DynamicPPL.Metadata{Dict{VarName{:γ, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:γ, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}}, Float64}, Model{typeof(InferenceModel), (:y, :t), (), (), Tuple{Vector{Float64}, Vector{Float64}}, Tuple{}, DefaultContext}, Nothing}, transition::ESSample{Vector{Walker{Vector{Float64}}}})\n",
      "    @ Turing.Inference ~/.julia/packages/Turing/r3Hmj/src/mcmc/abstractmcmc.jl:11\n",
      "  [7] transition_to_turing(f::LogDensityProblemsADForwardDiffExt.ForwardDiffLogDensity{LogDensityFunction{TypedVarInfo{@NamedTuple{α::DynamicPPL.Metadata{Dict{VarName{:α, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:α, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}, β::DynamicPPL.Metadata{Dict{VarName{:β, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:β, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}, γ::DynamicPPL.Metadata{Dict{VarName{:γ, typeof(identity)}, Int64}, Vector{Normal{Float64}}, Vector{VarName{:γ, typeof(identity)}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}}, Float64}, Model{typeof(InferenceModel), (:y, :t), (), (), Tuple{Vector{Float64}, Vector{Float64}}, Tuple{}, DefaultContext}, Nothing}, ForwardDiff.Chunk{3}, ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, ForwardDiff.GradientConfig{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DynamicPPL.DynamicPPLTag, Float64}, Float64, 3}}}}, transition::ESSample{Vector{Walker{Vector{Float64}}}})\n",
      "    @ Turing.Inference ~/.julia/packages/Turing/r3Hmj/src/mcmc/abstractmcmc.jl:17\n",
      "  [8] step(rng::Random.TaskLocalRNG, model::Model{typeof(InferenceModel), (:y, :t), (), (), Tuple{Vector{Float64}, Vector{Float64}}, Tuple{}, DefaultContext}, sampler_wrapper::Sampler{Turing.Inference.ExternalSampler{EnsembleSliceSampler{Float64, Int64}, AutoForwardDiff{nothing, Nothing}, true}}; initial_state::Nothing, initial_params::Nothing, kwargs::@Kwargs{})\n",
      "    @ Turing.Inference ~/.julia/packages/Turing/r3Hmj/src/mcmc/abstractmcmc.jl:141\n",
      "  [9] step\n",
      "    @ ~/.julia/packages/Turing/r3Hmj/src/mcmc/abstractmcmc.jl:96 [inlined]\n",
      " [10] macro expansion\n",
      "    @ ~/.julia/packages/AbstractMCMC/YrmkI/src/sample.jl:130 [inlined]\n",
      " [11] macro expansion\n",
      "    @ ~/.julia/packages/ProgressLogging/6KXlp/src/ProgressLogging.jl:328 [inlined]\n",
      " [12] (::AbstractMCMC.var\"#22#23\"{Bool, String, Nothing, Int64, Int64, Nothing, @Kwargs{}, Random.TaskLocalRNG, Model{typeof(InferenceModel), (:y, :t), (), (), Tuple{Vector{Float64}, Vector{Float64}}, Tuple{}, DefaultContext}, Sampler{Turing.Inference.ExternalSampler{EnsembleSliceSampler{Float64, Int64}, AutoForwardDiff{nothing, Nothing}, true}}, Int64, Int64})()\n",
      "    @ AbstractMCMC ~/.julia/packages/AbstractMCMC/YrmkI/src/logging.jl:12\n",
      " [13] with_logstate(f::Function, logstate::Any)\n",
      "    @ Base.CoreLogging ./logging.jl:515\n",
      " [14] with_logger(f::Function, logger::LoggingExtras.TeeLogger{Tuple{LoggingExtras.EarlyFilteredLogger{TerminalLoggers.TerminalLogger, AbstractMCMC.var\"#1#3\"{Module}}, LoggingExtras.EarlyFilteredLogger{Base.CoreLogging.SimpleLogger, AbstractMCMC.var\"#2#4\"{Module}}}})\n",
      "    @ Base.CoreLogging ./logging.jl:627\n",
      " [15] with_progresslogger(f::Function, _module::Module, logger::Base.CoreLogging.SimpleLogger)\n",
      "    @ AbstractMCMC ~/.julia/packages/AbstractMCMC/YrmkI/src/logging.jl:36\n",
      " [16] macro expansion\n",
      "    @ ~/.julia/packages/AbstractMCMC/YrmkI/src/logging.jl:11 [inlined]\n",
      " [17] mcmcsample(rng::Random.TaskLocalRNG, model::Model{typeof(InferenceModel), (:y, :t), (), (), Tuple{Vector{Float64}, Vector{Float64}}, Tuple{}, DefaultContext}, sampler::Sampler{Turing.Inference.ExternalSampler{EnsembleSliceSampler{Float64, Int64}, AutoForwardDiff{nothing, Nothing}, true}}, N::Int64; progress::Bool, progressname::String, callback::Nothing, discard_initial::Int64, thinning::Int64, chain_type::Type, initial_state::Nothing, kwargs::@Kwargs{})\n",
      "    @ AbstractMCMC ~/.julia/packages/AbstractMCMC/YrmkI/src/sample.jl:120\n",
      " [18] sample(rng::Random.TaskLocalRNG, model::Model{typeof(InferenceModel), (:y, :t), (), (), Tuple{Vector{Float64}, Vector{Float64}}, Tuple{}, DefaultContext}, sampler::Sampler{Turing.Inference.ExternalSampler{EnsembleSliceSampler{Float64, Int64}, AutoForwardDiff{nothing, Nothing}, true}}, N::Int64; chain_type::Type, resume_from::Nothing, initial_state::Nothing, kwargs::@Kwargs{})\n",
      "    @ DynamicPPL ~/.julia/packages/DynamicPPL/DvdZw/src/sampler.jl:93\n",
      " [19] sample\n",
      "    @ ~/.julia/packages/DynamicPPL/DvdZw/src/sampler.jl:83 [inlined]\n",
      " [20] #sample#4\n",
      "    @ ~/.julia/packages/Turing/r3Hmj/src/mcmc/Inference.jl:274 [inlined]\n",
      " [21] sample\n",
      "    @ ~/.julia/packages/Turing/r3Hmj/src/mcmc/Inference.jl:265 [inlined]\n",
      " [22] #sample#3\n",
      "    @ ~/.julia/packages/Turing/r3Hmj/src/mcmc/Inference.jl:262 [inlined]\n",
      " [23] sample(model::Model{typeof(InferenceModel), (:y, :t), (), (), Tuple{Vector{Float64}, Vector{Float64}}, Tuple{}, DefaultContext}, alg::Turing.Inference.ExternalSampler{EnsembleSliceSampler{Float64, Int64}, AutoForwardDiff{nothing, Nothing}, true}, N::Int64)\n",
      "    @ Turing.Inference ~/.julia/packages/Turing/r3Hmj/src/mcmc/Inference.jl:256\n",
      " [24] top-level scope\n",
      "    @ ~/github/Bac_a_sable_Monte_Carlo/julia/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X26sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "chain = sample(mymod, externalsampler(sampler), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_matrix = stack(sample -> sample.x, samples)\n",
    "ch = Chains(permutedims(samples_matrix, [3, 2, 1]),[:α,:β,:γ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch[100:end,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using StatsPlots\n",
    "plot(ch[100:end,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using DelimitedFiles\n",
    "# data = readdlm(\"data.txt\")\n",
    "# t, y, yerr = data[:, 1], data[:, 2], data[:, 3]\n",
    "\n",
    "\n",
    "# Turing.Inference.getparams(::Turing.Model, sample::ESSample) = sample.x\n",
    "\n",
    "# fun(x,α,β,γ) = @. α * x^2 + β * x + γ\n",
    "# x = 0.0:0.5:15.0\n",
    "\n",
    "# @model function InferenceModel(y)\n",
    "# \tα ~ Normal(2, 1)\n",
    "# \tβ ~ Normal(0, 1)\n",
    "# \tγ ~ Normal(-2, 1)\n",
    "\n",
    "# \ty ~ MvNormal(fun(x, α, β, γ), 1)\n",
    "# end\n",
    "\n",
    "# m = InferenceModel(y)\n",
    "# m2 = m | (α=1.0, β=0.3, γ=-2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DynamicPPL.SampleFromPrior()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
